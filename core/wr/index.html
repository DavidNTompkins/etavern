<!doctype html>

<html lang="en">
<head>
  <meta charset="utf-8">

  <title>eTavern</title>
  <meta name="description" content="the seediest watering hole on the web">
  <meta name="author" content="David Tompkins">

  
  <script src="https://rtcmulticonnection.herokuapp.com/dist/RTCMultiConnection.min.js"></script>
  <script src="https://rtcmulticonnection.herokuapp.com/socket.io/socket.io.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js"></script>

  <script src="../../third_party/gum-av.js" type="module"></script>
  <!-- <script src="main.js" type="module"></script> -->
  <style>
  .list-item {
        display: inline-block;
        margin: 1em;
        padding: 1em;
        box-shadow: 1px 2px 4px 0px rgba(0,0,0,0.25);
      }
  .canvas {
    width=1000;
  }
  </style>
  
</head>

<body>



  <button id="btn-open-room">Open Room</button>
  <button id="btn-join-room">Join Room</button><hr>
  <!-- buttons above from rtc, section below from facemesh -->
  <p id="status">Loading...</p>
    <div id="container">
      <canvas id="canvas"></canvas>
      <gum-av style="display:none"></gum-av>
    </div>

<script type="module">
//Loading some libaries and tools (mostly from facemesh)
import {
  WebGLRenderer,
  PCFSoftShadowMap,
  sRGBEncoding,
  Scene,
  SpotLight,
  PerspectiveCamera,
  HemisphereLight,
  AmbientLight,
  IcosahedronGeometry,
  OrthographicCamera,
  DoubleSide,
  Mesh,
  TextureLoader,
  MeshBasicMaterial,
  MeshStandardMaterial,
  Texture,
} from "../../third_party/three.module.js";
import { FaceMeshFaceGeometry } from "../../js/face.js";
import { OrbitControls } from "../../third_party/OrbitControls.js";


//some dummy variables until I get everything integrated
let local_username="Hogs"
let local_texture="Rabbits"
let local_facemesh="Bananas"
const facemeshes =[];

// ----------------------------------------
// Code for initializing data + sound only connection 
var connection = new RTCMultiConnection();
connection.socketURL = 'https://rtcmulticonnection.herokuapp.com:443/'; //should look into getting my own
connection.session = {
    audio: true,
    data: true,
    video: false
};
connection.mediaConstraints = {
    audio: true,
    video: false
};
connection.sdpConstraints.mandatory = {
    OfferToReceiveAudio: true,
    OfferToReceiveVideo: false
};
// This code adds in the audio playback. Should work to make it hidden. 
connection.onstream = function(event) {
    document.body.appendChild( event.mediaElement );
    //Here's where we should begin appending face mesh divs. 
    add_screen("temp","yep");
};



// ----------------------------------------
// UI logic - needs to change to either generate tavern names or accept existing
var predefinedRoomId = 'etavern';

document.getElementById('btn-open-room').onclick = function() {
    this.disabled = true;
    connection.open( predefinedRoomId );
};

document.getElementById('btn-join-room').onclick = function() {
    this.disabled = true;
    connection.join( predefinedRoomId );
};
// ----------------------------------------
// face generation logic:
const av = document.querySelector("gum-av");
const canvas = document.querySelector("canvas");
const status = document.querySelector("#status");

// Set a background color, or change alpha to false for a solid canvas.
const renderer = new WebGLRenderer({ antialias: true, alpha: true, canvas });
// renderer.setClearColor(0x202020);
renderer.setPixelRatio(window.devicePixelRatio);
renderer.shadowMap.enabled = true;
renderer.shadowMap.type = PCFSoftShadowMap;
renderer.outputEncoding = sRGBEncoding;

const scene = new Scene();
const camera = new OrthographicCamera(1, 1, 1, 1, -1000, 1000);

// Change to renderer.render(scene, debugCamera); for interactive view.
/*
const debugCamera = new PerspectiveCamera(75, 1, 0.1, 1000);
debugCamera.position.set(300, 300, 300);
debugCamera.lookAt(scene.position);
const controls = new OrbitControls(debugCamera, renderer.domElement);
*/
let width = 0;
let height = 0;

function resize() {
  const videoAspectRatio = width / height;
  const windowWidth = window.innerWidth;
  const windowHeight = window.innerHeight;
  const windowAspectRatio = windowWidth / windowHeight;
  let adjustedWidth;
  let adjustedHeight;
   if (videoAspectRatio > windowAspectRatio) {
    adjustedWidth = windowWidth;
    adjustedHeight = windowWidth / videoAspectRatio;
  } else {
    adjustedWidth = windowHeight * videoAspectRatio;
    adjustedHeight = windowHeight;
  }
  renderer.setSize(adjustedWidth, adjustedHeight); 
  //debugCamera.aspect = videoAspectRatio;
  //debugCamera.updateProjectionMatrix();
}

window.addEventListener("resize", () => {
  resize();
});
resize();
//renderer.render(scenes[0], camera);

// Create a loader.
const loader = new TextureLoader();

// Create wireframe material for debugging.
const wireframeMaterial = new MeshBasicMaterial({
  color: 0xff00ff,
  wireframe: true,
  transparent: true,
  opacity: 0.5,
});

// Create material for mask.
const material = new MeshStandardMaterial({
  color: 0xffffff,
  roughness: 0.8,
  metalness: 0,
  map: null, // Set later by the face detector.
  transparent: true,
  side: DoubleSide,
  opacity: 1,
});

/*// Create a new geometry helper.
const faceGeometry = new FaceMeshFaceGeometry({normalizeCoords: true});

// Create mask mesh.
const mask = new Mesh(faceGeometry, material);
scene.add(mask);
mask.receiveShadow = mask.castShadow = true;
//mask.position.set(200,200,200); //weirdly does not keep graphics - may need to shift texture as well

// Add lights.
const spotLight = new SpotLight(0xffffff, 0.5);
spotLight.position.set(0.5, 0.5, 1);
spotLight.position.multiplyScalar(400);
scene.add(spotLight);

spotLight.castShadow = true;

spotLight.shadow.mapSize.width = 1024;
spotLight.shadow.mapSize.height = 1024;

spotLight.shadow.camera.near = 200;
spotLight.shadow.camera.far = 800;

spotLight.shadow.camera.fov = 40;

spotLight.shadow.bias = -0.005;

scene.add(spotLight);

const hemiLight = new HemisphereLight(0xffffbb, 0x080820, 0.25);
//scene.add(hemiLight);

const ambientLight = new AmbientLight(0x404040, 0.5);
scene.add(ambientLight); */

// Defines if the source should be flipped horizontally.
let flipCamera = true;

let referenceFace;

//--- This function is the main loop. Gets model data and renders own face. 
async function render(model) {
  // Wait for video to be ready (loadeddata).
  await av.ready();
  // Flip video element horizontally if necessary.
  av.video.style.transform = flipCamera ? "scaleX(-1)" : "scaleX(1)";
  av.video.style.display = "none"

  // Resize orthographic camera to video dimensions if necessary.
  if (width !== av.video.videoWidth || height !== av.video.videoHeight) {
    const w = av.video.videoWidth;
    const h = av.video.videoHeight;
    camera.left = -0.5 * w;
    camera.right = 0.5 * w;
    camera.top = 0.5 * h;
    camera.bottom = -0.5 * h;
    camera.updateProjectionMatrix();
    width = w;
    height = h;
    resize();
    facemeshes[0].setSize(w, h); // DT - this appears to be the trick to getting the camera to fit the zoom windows.
  } 
  // Wait for the model to return a face.
  const faces = await model.estimateFaces(av.video, false, flipCamera);

  av.style.opacity = 1;

  // There's at least one face.
  if (faces.length > 0) {
      // here's where I'm going to try to send data: 
    connection.send({
      username: local_username,
      texture: local_texture,
      facemesh: faces[0]
    });
    //console.log(faces[0])
    

    // Update face mesh geometry with new data. 
    // ---- This one updates the same face
    facemeshes[0].update(faces[0], flipCamera);

    // Use the reference face texture coordinates for this face geometry.
    for (let j = 0; j < 468; j++) {
      let x = referenceFace.face.scaledMesh[j][0];
      let y = referenceFace.face.scaledMesh[j][1];
      facemeshes[0].uvs[j * 2] = x;
      facemeshes[0].uvs[j * 2 + 1] = 1 - y;
    }
    facemeshes[0].getAttribute("uv").needsUpdate = true;
  

}
  // below actually renders
  renderer.render(scene, camera);
  console.log("allegedly rendered own face")

  requestAnimationFrame(() => render(model));
}

// Tries to find a face in an image so we can texure map it into the life feed face.
async function getFace(model, texture) {
  const faces = await model.estimateFaces(texture.image);
  if (!faces.length) {
    status.textContent = "No face detected! Try another image.";
    throw new Error("No face detected!");
  }
  // Get the found face and turn into texture coordinates.
  const face = faces[0];
  for (let j = 0; j < face.scaledMesh.length; j++) {
    face.scaledMesh[j][0] /= texture.image.naturalWidth;
    face.scaledMesh[j][1] /= texture.image.naturalHeight;
  }
  return { texture, face: faces[0] };
}

// We need a separate model because they can't be reused (?).
let modelRef;

// Init the demo, loading dependencies.
async function init() {
  await Promise.all([tf.setBackend("webgl"), av.ready()]);
  status.textContent = "Loading model...";
  let texture, model;
  [texture, model, modelRef] = await Promise.all([
    loader.loadAsync("../../assets/ao.jpg"),
    facemesh.load({ maxFaces: 1 }),
    facemesh.load({ maxFaces: 1 }),
  ]);
  try {
    referenceFace = await getFace(modelRef, texture);
  } catch (e) {
    console.error(e);
    return;
  }
  material.map = referenceFace.texture;
  status.textContent = "Detecting face...";
  add_screen("temp","yep");
  await render(model);
  status.textContent = "Drop an image into the page.";

}

// Handles dropping an image.
async function dropHandler(ev) {
  ev.preventDefault();
  status.textContent = "Analysing...";

  if (ev.dataTransfer.items) {
    for (let item of ev.dataTransfer.items) {
      if (item.kind === "file") {
        var file = item.getAsFile();
        modelRef = await facemesh.load({ maxFaces: 1 });
        const url = URL.createObjectURL(file);
        let texture = await loader.loadAsync(url);
        try {
          referenceFace = await getFace(modelRef, texture);
        } catch (e) {
          console.error(e);
          return;
        }
        material.map = referenceFace.texture;
        material.needsUpdate = true;
        status.textContent = "";
      }
    }
  } else {
    for (let file of ev.dataTransfer.files) {
    }
  }
}
// ----------------------------------------
// This function handles creating new facemeshes. Name is legacy, should probably be updated to represent truth (adds meshes not screens)
function add_screen(username, texture){
  // a lot of this comes from the multiple elements THREE.js example.
  //canvas = document.getElementById( "c" ); // already defined above.

  // this block creates the initial face mesh geometry
  // a lot of this comes from the FaceMeshFaceGeometry face transfer example
  const faceGeometry = new FaceMeshFaceGeometry({normalizeCoords: true});
  const mask = new Mesh(faceGeometry, material);
  scene.add(mask);

  // this block adds lighting and puts the scene into the scenes array for easy handling
  // this should be swapped with the automated face matching texture code

  //faceGeometry.translate({x: 200*facemeshes.length,y:0,z:0});
  facemeshes.push(faceGeometry);
   
}


connection.onmessage = function(event) {
      facemesh = event.data.facemesh;
      //console.log(facemesh)
      facemeshes[1].update(facemesh, flipCamera)
      facemeshes[1].translate({x: 200,y:0,z:0});
      renderer.render(scene, camera);
    };

function dragOverHandler(ev) {
  ev.preventDefault();
}

renderer.domElement.addEventListener("drop", dropHandler);
renderer.domElement.addEventListener("dragover", dragOverHandler);

init();


</script>

</body>
</html>