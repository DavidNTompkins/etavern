<!doctype html>

<html lang="en">
<head>
  <meta charset="utf-8">

  <title>David-Chat</title>
  <meta name="description" content="We're all David here">
  <meta name="author" content="David Tompkins">

  
  <script src="../../js/RTCMultiConnection.min.js"></script>
  <script src="https://rtcmulticonnection.herokuapp.com/socket.io/socket.io.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-wasm/dist/tf-backend-wasm.js"></script>
  <script src="../../js/FileBufferReader.js"></script>

  <script src="../../third_party/gum-av.js" type="module"></script>
  <!-- <script src="main.js" type="module"></script> -->
  <style>
      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      gum-av,
      canvas {
        position: absolute;
        left: 0;
        top: 0;
        right: 0;
        bottom: 0;
        width: 100%;
        height: 100%;
      }

      canvas {
        margin: auto;
      }

      #container {
        position: absolute;
        left: 0;
        top: 1;
        width: 100%;
        height: 100%;
        background-color: #60798E;
      }

      #status {
        position: relative;
        left: 0;
        top: 0;
        pointer-events: none;
        color: white;
        padding: 1em;
        font-family: roboto, sans-serif;
        z-index: 1;
        background: gray;
      }
      button{
        z-index: 15;
      }

      gum-av {
        font-family: roboto, sans-serif;
        color: white;
        opacity: 0;
        transition: opacity 1s ease-out;
      }
    </style>
  
</head>

<body>



  <button id="btn-open-room">Open Room</button>
  <button id="btn-join-room">Join Room</button>
  <input type="text" id="img_origin" value="Put an URL here">
  <button id="load_a_face">New Look</button>
  <hr>
  <!-- buttons above from rtc, section below from facemesh -->
  <p id="status">Loading...</p>
    <div id="container">
      <canvas id="canvas"></canvas>
      <gum-av style="display:none"></gum-av>
    </div>

<script type="module">
//Loading some libaries and tools (mostly from facemesh)
import {
  WebGLRenderer,
  PCFSoftShadowMap,
  sRGBEncoding,
  Scene,
  SpotLight,
  PerspectiveCamera,
  HemisphereLight,
  AmbientLight,
  IcosahedronGeometry,
  OrthographicCamera,
  DoubleSide,
  Mesh,
  TextureLoader,
  MeshBasicMaterial,
  MeshStandardMaterial,
  Texture,
  Matrix4,
} from "../../third_party/three.module.js";
import { FaceMeshFaceGeometry } from "../../js/face.js";
import { OrbitControls } from "../../third_party/OrbitControls.js";


//reserving some variables
let local_username=""
let local_texture="Rabbits"
let local_facemesh=""
let facemeshes = new Map();
let referenceFaces = new Map();
let external_faces = new Map(); // (allows me to only update external faces with most recent face (hopefully))
let materials = new Map();
let update_flags = new Map();
let connected_flag = false;
let receive_flag = false; // not currently used, but might be good to implement
const max_buff = 1; //named poorly, does not refer to me
let model, modelRef;
let default_face;
const min_send_period = 50 // ms - min time before resending a face item
let send_flag = false;

// ----------------------------------------
// Code for initializing data + sound only connection 
var connection = new RTCMultiConnection();
connection.socketURL = 'https://rtcmulticonnection.herokuapp.com:443/'; //should look into getting my own
connection.session = {
    audio: true,
    data: true,
    video: false
};
connection.mediaConstraints = {
    audio: true,
    video: false
};
connection.sdpConstraints.mandatory = {
    OfferToReceiveAudio: true,
    OfferToReceiveVideo: false
};
// This code adds in the audio playback. Should work to make it hidden. 
connection.onstream = function(event) {
    document.body.appendChild( event.mediaElement );
};

connection.enableFileSharing = true; // we're not sending files, but the facemesh is an arraybuffer and gets caught on the same flag.

connection.onPeerStateChanged = function(state) {
    if (state.iceConnectionState=='connected') {
      connected_flag = true;
    }
};
connection.extra.img_origin= "../../assets/dnt32.jpg";
connection.socketCustomParameters = '&extra=' + JSON.stringify(connection.extra);

connection.onExtraDataUpdated = function(event) {
  var new_origin = event.extra.img_origin;
  var whoModified = event.userid; // or event.extra.fullName
  if(facemeshes.get(whoModified)){
    load_new_face(new_origin,whoModified);
    //console.log("New Face received")
  };
};

var predefinedRoomId = 'etavern';

document.getElementById('btn-open-room').onclick = function() {
    this.disabled = true;
    connection.open( predefinedRoomId );
};

document.getElementById('btn-join-room').onclick = function() {
    this.disabled = true;
    connection.join( predefinedRoomId );
};

document.getElementById('load_a_face').onclick = async function() {
    load_new_face(document.getElementById("img_origin").value,"myself");
    connection.extra.img_origin= document.getElementById("img_origin").value;
    connection.updateExtraData();
};

setInterval(() => send_flag=true, min_send_period);

// ----------------------------------------
// face generation logic:
const windowWidth = window.innerWidth;
const windowHeight = window.innerHeight;
const av = document.querySelector("gum-av");
const canvas = document.querySelector("canvas");
const status = document.querySelector("#status");
canvas.width = windowWidth;
canvas.height = windowHeight;

// Set a background color, or change alpha to false for a solid canvas.
const renderer = new WebGLRenderer({ antialias: true, alpha: true, canvas });
renderer.setPixelRatio(window.devicePixelRatio);
renderer.shadowMap.enabled = true;
renderer.shadowMap.type = PCFSoftShadowMap;
renderer.outputEncoding = sRGBEncoding;

const scene = new Scene();

let width =0;
let height = 0;
const camera = new OrthographicCamera(-0.5*windowWidth, 0.5*windowWidth, 0.5*windowHeight, -0.5*windowHeight, -1000, 1000);

function resize() {
    const w = window.innerWidth;
    const h = window.innerHeight;
    camera.left = -0.5 * w;
    camera.right = 0.5 * w;
    camera.top = 0.5 * h;
    camera.bottom = -0.5 * h;
    camera.updateProjectionMatrix();
    width = w;
    height = h;
    renderer.setSize(width, height);
    canvas.width = width;
    canvas.height = height;
}
  
window.addEventListener("resize", () => {
  resize();
}); 

resize();

// Create a loader.
const loader = new TextureLoader();

// Create wireframe material for debugging.
const wireframeMaterial = new MeshBasicMaterial({
  color: 0xff00ff,
  wireframe: true,
  transparent: true,
  opacity: 0.5,
});

// Add lights.
const spotLight = new SpotLight(0xffffff, 0.5);
spotLight.position.set(0.5, 0.5, 1);
spotLight.position.multiplyScalar(400);
spotLight.angle=(Math.PI/2);
scene.add(spotLight);
spotLight.castShadow = true;
spotLight.shadow.mapSize.width = 1024;
spotLight.shadow.mapSize.height = 1024;
spotLight.shadow.camera.near = 200;
spotLight.shadow.camera.far = 800;
spotLight.shadow.camera.fov = 40;
spotLight.shadow.bias = -0.005;
scene.add(spotLight);

const hemiLight = new HemisphereLight(0xffffbb, 0x080820, 0.25);
//scene.add(hemiLight);

const ambientLight = new AmbientLight(0x404040, 0.5);
scene.add(ambientLight); 

// Defines if the source should be flipped horizontally.
let flipCamera = true;

let referenceFace;

//--- This function is the main loop. Gets model data and renders own face. 
async function render(model) {
  // Wait for video to be ready (loadeddata).
  await av.ready();
  // Flip video element horizontally if necessary.
  const windowWidth = window.innerWidth;
  const windowHeight = window.innerHeight;
  av.video.style.transform = flipCamera ? "scaleX(-1)" : "scaleX(1)";
  av.video.style.display = "none";

  // Wait for the model to return a face.
  const faces = await model.estimateFaces(av.video, false, flipCamera);

  av.style.opacity = 1;

  // There's at least one face.
  if (faces.length > 0) {
      // here's where I'm going to try to send data: 
     if(connected_flag&&send_flag){  //was a flag instead
      //console.log("message sent")
      let typed_array = Float64Array.from(faces[0].scaledMesh.flat());
      //console.log(typed_array.buffer); 
      connection.send(typed_array.buffer); //sending only the arraybuffer
      /*connection.send({
        inbound_facemesh: faces[0].scaledMesh
     });*/
      send_flag=false;
    };
    

    // Update face mesh geometry with new data. 
    // ---- This one updates the same face
    facemeshes.get("myself").update(faces[0].scaledMesh, flipCamera);
    for (let entry of external_faces.entries()){
      if (update_flags.get(entry[0])) {
        facemeshes.get(entry[0]).update(entry[1], flipCamera);
      };
    }; 
    let k = 0
    for (let entry of referenceFaces.entries()){
      if (update_flags.get(entry[0])) {
        for (let j = 0; j < 468; j++) {
          let x = entry[1].face.scaledMesh[j][0]; // entry[1] was referenceFaces.get(entry[0])under facemeshes iteration
          let y = entry[1].face.scaledMesh[j][1];
          facemeshes.get(entry[0]).uvs[j * 2] = x; //value was facemeshes[k]
          facemeshes.get(entry[0]).uvs[j * 2 + 1] = 1 - y;
        }
        facemeshes.get(entry[0]).getAttribute("uv").needsUpdate = true;
        const x_origin =(0.6-(((k+1)%2)*0.6));
        const y_origin =(1.4-Math.ceil(0.5*(k+1))*0.5)
        facemeshes.get(entry[0]).setSize((x_origin*windowWidth), (y_origin*windowHeight)); 
      }
      k+=1;
      };
}
  // below actually renders
  renderer.render(scene, camera);
  
  requestAnimationFrame(() => render(model));
}

// Tries to find a face in an image so we can texure map it into the life feed face.
async function getFace(model, texture) {
  const faces = await model.estimateFaces(texture.image);
  if (!faces.length) {
    status.textContent = "No face detected! Try another image.";
    throw new Error("No face detected!");
  }
  // Get the found face and turn into texture coordinates.
  const face = faces[0];
  for (let j = 0; j < face.scaledMesh.length; j++) {
    face.scaledMesh[j][0] /= texture.image.naturalWidth;
    face.scaledMesh[j][1] /= texture.image.naturalHeight;
  }
  return { texture, face: faces[0] };
}

// Init the demo, loading dependencies.
async function init() {
  await Promise.all([tf.setBackend("webgl"), av.ready()]);
  status.textContent = "Loading model...";
  let default_texture;
  [default_texture, model, modelRef] = await Promise.all([
    loader.loadAsync("../../assets/dnt32.jpg"),
    facemesh.load({ maxFaces: 1 }),
    facemesh.load({ maxFaces: 1 }),
  ]);
  try {
    default_face= await getFace(modelRef, default_texture);
  } catch (e) {
    console.error(e);
    return;
  }
  //material.map = referenceFace.texture;
  status.textContent = "Detecting face...";
  update_flags.set("myself",true);
  add_screen("myself","../../assets/dnt32.jpg");//referenceFaces.get("myself").texture);
  await render(model);
  status.textContent = "Drop an image into the page.";

}

async function load_new_face(origin,username) {
  status.textContent = "New Face, New Me"
  let temp_ref_face;
  temp_ref_face = await getFace(modelRef, await loader.loadAsync(origin));
  temp_ref_face = await getFace(modelRef, await loader.loadAsync(origin));
  try {
    referenceFaces.set(username, temp_ref_face);
  } catch (e) {
    status.textContent = "Try a different image, or check your link"
    console.error(e);
    return;
  }
  materials.get(username).map = temp_ref_face.texture;
  materials.get(username).needsUpdate = true;
}


// Handles dropping an image.
async function dropHandler(ev) {
  ev.preventDefault();
  status.textContent = "Analysing...";

  if (ev.dataTransfer.items) {
    for (let item of ev.dataTransfer.items) {
      if (item.kind === "file") {
        var file = item.getAsFile();
        //modelRef = await facemesh.load({ maxFaces: 1 });
        const url = URL.createObjectURL(file);
        let texture = await loader.loadAsync(url);
        try {
          referenceFaces.set("myself", await getFace(modelRef, texture));
        } catch (e) {
          console.error(e);
          return;
        }
        materials.get("myself").map = referenceFaces.get("myself").texture;
        materials.get("myself").needsUpdate = true;
        status.textContent = "";
      }
    }
  } else {
    for (let file of ev.dataTransfer.files) {
    }
  }
}
// ----------------------------------------
// This function handles creating new facemeshes. Name is legacy, should probably be updated to represent truth (adds meshes not screens)
function add_screen(username, origin){
    // Create material for mask.
  const material = new MeshStandardMaterial({
    color: 0xffffff,
    roughness: 0.8,
    metalness: 0,
    map: null, // Set later by the face detector.
    transparent: true,
    side: DoubleSide,
    opacity: 1,
  });
  referenceFaces.set(username,default_face); 

  materials.set(username,material);
  materials.get(username).map = referenceFaces.get(username).texture;

  // a lot of this comes from the FaceMeshFaceGeometry face transfer example
  const new_faceGeometry = new FaceMeshFaceGeometry({normalizeCoords: true});
  const mask = new Mesh(new_faceGeometry, materials.get(username));
  mask.name = username;
  scene.add(mask);
  load_new_face(origin,username);  

  if (facemeshes.size>0) {
    new_faceGeometry.setSize(canvas.width,canvas.height);
  }else{
    new_faceGeometry.setSize(canvas.width,canvas.height);
  };
  facemeshes.set(username,new_faceGeometry);
   
}
connection.onleave = function(event) {
  console.log("QUITTER");
  var exfriend = event.userid;
  var exfriends_face = scene.getObjectByName(exfriend);
  scene.remove(exfriends_face);
}


connection.onmessage = function(event) {
  //console.log(event.data);   
  const raw_facemesh = new Float64Array(event.data);//event.data.inbound_facemesh;
 
  var inbound_facemesh =[];
  for (var i = 0; i <468; i++) {
    for (var j=0; i<3; i++) {
      inbound_facemesh[i][j] =raw_facemesh[(3*i+j)];
    };
  };
  console.log("inbound_facemesh");
  const array_size = 3;
 // while ()
  
  const username = event.userid;
  const inbound_img_origin =event.extra.img_origin
  // attempting to update if have the info to do so, but not if we don't
  if (facemeshes.has(username)){
    external_faces.set(username,inbound_facemesh);
    update_flags.set(username,true);
  }else if (!(facemeshes.has(username))){
    update_flags.set(username,true);
    connected_flag= true;
   console.log("new user");
    add_screen(username,inbound_img_origin);
  };
    };

function dragOverHandler(ev) {
  ev.preventDefault();
}

renderer.domElement.addEventListener("drop", dropHandler);
renderer.domElement.addEventListener("dragover", dragOverHandler);

init();


</script>

</body>
</html>